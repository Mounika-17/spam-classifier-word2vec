{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba73349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef03232",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "1. It is a python library which provides a built-in models like  ready-to-use implementation of Word2Vec —you don’t have to build the neural network from scratch.\n",
    "2. We can try the already built-in models like Word2Vec, GloVe, or FastText using gensim.downloader.api.load()\n",
    "3. If we want to train our own embeddings we can do that - from gensim.models import Word2Vec\n",
    "4. There are different word2vec models. If we want to create our own embedding model we need to import the Word2Vec class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ad2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd39ff",
   "metadata": {},
   "source": [
    "#### Let's see how we can load the google-news word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d998717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156fe50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the model give an output for a word\n",
    "# Google news word2vec model has 300 dimensions and it is trained on 3 million words/phrases. 300 dimensions means each word is represented as a vector of 300 elements.\n",
    "# Let's see the vector for the word 'king'\n",
    "vector = model['king']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_length = len(vector)\n",
    "print(f\"Length of the vector for the word 'king': {vector_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692fec07",
   "metadata": {},
   "source": [
    "#### Let's create our own Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe456e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the data folder\n",
    "import pandas as pd\n",
    "file_path = os.path.join('..', 'data', 'SMSSpamCollection.txt')\n",
    "messages=pd.read_csv(file_path, sep='\\t', names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ee748",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the data cleaning and preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prepares text for CountVectorizer or TfidfVectorizer, which expect string sentences, not token lists.\n",
    "# But for Word2Vec, we need list of token lists. That is done in the next cells.\n",
    "corpus=[]\n",
    "for i in range(0,len(messages)):\n",
    "    # [^a-zA-Z] → “anything not (^) an uppercase (A–Z) or lowercase (a–z) letter”.THis this removes all digits (0–9), punctuation, symbols, etc. Replace them with spaces from message column\n",
    "    # If we need numbers we can use [^a-zA-Z0-9]\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    # Split the sentences in to words by spaces\n",
    "    review = review.split()\n",
    "    # Apply the stemming\n",
    "    review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    # Join the words to make sentences\n",
    "    # If review is empty → ' '.join([]) → '' (an empty string).\n",
    "    review = ' '.join(review)\n",
    "    # Append the snetence to corpus\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34970e",
   "metadata": {},
   "source": [
    "#### Meaning of the below list comprehensino\n",
    "| Expression          | Meaning                                                         |\n",
    "| ------------------- | --------------------------------------------------------------- |\n",
    "| `map(len, corpus)`  | Gets length of each cleaned message                             |\n",
    "| `zip(...)`          | Pairs lengths, cleaned messages, and original messages together |\n",
    "| `if i < 1`          | Filters messages with empty cleaned text                        |\n",
    "| `[ [i, j, k] ... ]` | Builds list showing length, cleaned text, and original text     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if there are any empty messages in the corpus. \n",
    "# This helps in identifying the messages became empty after cleaning (e.g., removing stopwords, punctuation, etc.)\n",
    "[[i,j,k] for i,j,k in zip(list(map(len,corpus)),corpus, messages['message']) if i<1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048b749",
   "metadata": {},
   "source": [
    "#### empty strings '' can appear in corpus if the original message has no letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861197a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the complete details which original messages became empty after cleaning\n",
    "for i, j, k in zip(list(map(len, corpus)), corpus, messages['message']):\n",
    "    if i < 1:\n",
    "        print(f\"Original: {k}\")\n",
    "        print(f\"Cleaned: '{j}'\")\n",
    "        print(f\"Length: {i}\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49647c8",
   "metadata": {},
   "source": [
    "#### The 3 empty strings with white spaces that are after cleaning of the corpus will not be considered in the words. So total size will be reduced from 5572 to 5569 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994632f",
   "metadata": {},
   "source": [
    "##### How the below logic works?\n",
    "| Case    | Will it be added to `words`?                             | Reason                                        |\n",
    "| ------- | -------------------------------------------------------- | --------------------------------------------- |\n",
    "| `''`    | ❌ No                                                     | `sent_tokenize` → `[]`                        |\n",
    "| `'   '` | ❌ No (inner loop skips) or empty list if passed directly | No valid sentence/tokens                      |\n",
    "| `'u'`   | ✅ Added as `[]` by default (filtered)                    | Single char → filtered out unless `min_len=1` |\n",
    "| `'hi'`  | ✅ Added as `['hi']`                                      | Valid token                                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each cleaned sentence into a list of tokenized words for Word2Vec training.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "words=[]\n",
    "for sent in corpus:\n",
    "    sent_token=sent_tokenize(sent)\n",
    "    for sent in sent_token:\n",
    "        words.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd9fdb",
   "metadata": {},
   "source": [
    "##### Below are the steps to check the empty strings in words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd186951",
   "metadata": {},
   "outputs": [],
   "source": [
    "any(len(w) == 0 for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count empties\n",
    "sum(len(w)==0 for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabe845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the indexes of the empty words\n",
    "[i for i,w in enumerate(words) if len(w)==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives the 3 empty strings with white spaces from the corpus which I mentioned previously.\n",
    "[i for i, x in enumerate(corpus) if x.strip() == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac30766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d26970",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [4291, 5170]:\n",
    "    print(f\"Index {i}: {repr(corpus[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17725793",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[4291]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58126e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[5170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7670a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you skip empty sentences in your corpus → words loop (like if len(tokens) > 0),you are not preserving one-to-one alignment between corpus and words. \n",
    "# So if words[4291] and words[5170] are empty, that tells us: these indices in words do not correspond to the same indices in corpus anymore.\n",
    "# If you want to check which corpus entries created empty words,you must loop together like this:\n",
    "empty_indices = []\n",
    "for i, sent in enumerate(corpus):\n",
    "    sent_token = sent_tokenize(sent)\n",
    "    for s in sent_token:\n",
    "        tokens = simple_preprocess(s)\n",
    "        if len(tokens) == 0:\n",
    "            empty_indices.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc34aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [4293, 5173]:\n",
    "    print(f\"Index {i}: {repr(corpus[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52d508",
   "metadata": {},
   "source": [
    "##### If you see in the above they are very short messages 'g w r' and 'u'. Each contains a one or a few single-letter “words”. \n",
    "What simple_preprocess() does:\n",
    "\n",
    "gensim.utils.simple_preprocess() is not just a basic split —\n",
    "1. it removes very short tokens by default.\n",
    "2. By design, it ignores tokens that:  \n",
    "\n",
    "Are shorter than 2 characters (default min_len=2)  \n",
    "\n",
    "Or longer than 15 characters (default max_len=15)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170ef4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d017e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get 5569 because 3 messages became empty after cleaning.\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe141b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train our own Word2Vec model.\n",
    "import gensim\n",
    "# By default,min_count is 5, so words which are having frequency less than 5 will be ignored.\n",
    "model=gensim.models.Word2Vec(words) # we can mention parameters like vector size, window size, min count etc. By default vector size is 100, window size is 5 and min count is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c4455",
   "metadata": {},
   "source": [
    "#### 🧩 1️⃣ “100 dimensions” — are those also the words in my vocabulary?\n",
    "1. ❌ No — the 100 dimensions are not words. They are number of numeric features that capture relationships (like gender, tense, topic, etc.)\n",
    "2. They are numerical features (latent semantic dimensions) that represent meaning or context of words — not actual words themselves.\n",
    "3. These 100 numbers don’t correspond to specific words.Instead, they describe abstract properties — like:\n",
    "\n",
    "a. masculine/feminine axis   \n",
    "b. royalty/commoner axis  \n",
    "c. age, emotion, topic, etc.  \n",
    "\n",
    "The model learns these patterns automatically while training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will give the count of the words in the vocabulary and their dimension(means no.of columns)\n",
    "# since we have not mentioned any parameters, by default vector size is 100. so number of columns is 100.\n",
    "model.wv.vectors.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c15e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get all the vocabulary words in the model\n",
    "# Since I mentioned min_count as 5, words which are having frequency less than 5 are ignored.\n",
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9209e5b",
   "metadata": {},
   "source": [
    "#### 🧩 2️⃣ Should model.wv.vectors.shape and model.corpus_count be the same?\n",
    "❌ No, they are not the same thing — and they almost never match.\n",
    "| Attribute                   | Meaning                                                    |\n",
    "| --------------------------- | ---------------------------------------------------------- |\n",
    "| `model.wv.vectors.shape[0]` | Number of **unique words in the vocabulary** (rows)        |\n",
    "| `model.corpus_count`        | Number of **sentences** (or “documents”) used for training |\n",
    "\n",
    "🧠 Analogy\n",
    "\n",
    "Think of Word2Vec as a language school:  \n",
    "\n",
    "corpus_count → how many sentences it studied.  \n",
    "\n",
    "wv.vectors.shape[0] → how many unique words it learned from them.  \n",
    "\n",
    "wv.vectors.shape[1] → how many traits each word has learned (like tone, tense, meaning).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It indicates the number of sentences (or “documents”) used for training\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.epochs means it tells how many times your Word2Vec model iterated over the entire training corpus during training\n",
    "model.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the similar words\n",
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['good'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b071d2",
   "metadata": {},
   "source": [
    "##### What is model.wv in Word2Vec?\n",
    "| Part       | Description                                                             |\n",
    "| ---------- | ----------------------------------------------------------------------- |\n",
    "| `model.wv` | **Word vectors** — this is your vocabulary + learned embeddings         |\n",
    "| `model`    | Full model (includes training settings, negative sampling tables, etc.) |\n",
    "\n",
    "✅ So yes — model.wv is the learned vocabulary of your Word2Vec model.\n",
    "1. It stores all unique words that appeared in your training corpus (subject to min_count).\n",
    "2. It also stores the vector representation for each of those words.\n",
    "\n",
    "##### How we can get the NaNs?\n",
    "If a sentence has no valid words in model.wv, like \"12345\", \"!!!\", \"###\", etc.\n",
    "Then the list vectors becomes empty: vectors=[]  \n",
    "and   \n",
    "\n",
    "np.mean(vector,axis=0) returns nan.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cde0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have 100 dimesions for every word of word[0] similarly for all words in the words list.We need to take average of all word vectors to represent the entire sentence.\n",
    "# model.wv- \n",
    "import numpy as np\n",
    "def avg_word2vec(words):\n",
    "    # Iterates through each word in the sentence and retrieves its corresponding word vector from the Word2Vec model (model.wv).Collects these vectors into a list called vectors.\n",
    "    vectors=[model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vectors)>0:\n",
    "       return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "       # Suppose if the words in the sentence are not present in the vocabulary of the model i.e model.wv, then the list vectors becomes empty and if we do np.mean(vectors, axis=0) it returns a Nan value. To avoid that we can return a zero vector of same dimension as the model's word vectors.\n",
    "       return np.zeros(model.vector_size) # Handle the case where no words are found in the model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f46fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2dc7a",
   "metadata": {},
   "source": [
    "##### tqdm:\n",
    "tqdm is a python library for progress bars. It shows you a real-time progress indicator in the console or notebook while loops are running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the avg_word2vec function to each sentence in the words list to get the average word vectors for all sentences.\n",
    "import numpy as np\n",
    "# X is a list of vectors\n",
    "X=[]\n",
    "for i in tqdm(range(len(words))):\n",
    "    X.append(avg_word2vec(words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e498a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)  # number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f864d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent features. \n",
    "# X is a list of vectors. Not compatible with ML models. so convert it into a 2D NumPy array.\n",
    "X_new=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= messages['label'].map({'ham': 0, 'spam': 1}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056856e4",
   "metadata": {},
   "source": [
    "#### Execution of the below code:\n",
    "| Step | Expression                          | What It Does                                                    | Result Type  |\n",
    "| ---- | ----------------------------------- | --------------------------------------------------------------- | ------------ |\n",
    "| 1    | `map(lambda x: len(x) > 0, corpus)` | Checks which corpus entries are non-empty                       | list of bool |\n",
    "| 2    | `messages[...]`                     | Filters DataFrame to keep only rows with non-empty cleaned text | DataFrame    |\n",
    "| 3    | `y['label']`                        | Selects the label column                                        | Series       |\n",
    "| 4    | `.map({'ham': 0, 'spam': 1})`       | Converts text labels to numbers                                 | Series (int) |\n",
    "| 5    | `.values`                           | Converts Series → NumPy array                                   | ndarray      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861df2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is considering all the sentences but after cleaning like applying the regular expression '[^a-zA-Z]' we got the total 5569 sentences lets add that filter to y.\n",
    "# messages is the dataframe with 2 columns message and label and y stores the updated dataframe  where the corresponding corpus entry was non-empty.\n",
    "y=messages[list(map(lambda x:len(x)>0, corpus))] # This selects only the rows where the corresponding corpus entry was non-empty.\n",
    "# From the filtered DataFrame y, we take the label column and replaces the string labels using a dictionary: 'ham' → 0 and 'spam' → 1\n",
    "# .values - this converts the pandas series to a numpy array.\n",
    "y= y['label'].map({'ham': 0, 'spam': 1}).values          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa903b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e95f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932073c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f727d9",
   "metadata": {},
   "source": [
    "#### Create Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output feature\n",
    "y = messages['label'].map({'ham': 0, 'spam': 1}).values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# For binary BOW model use binary=True in CountVectorizer and for limiting the features use max_features and ngram_range parameter is used to consider more than one words together\n",
    "cv = CountVectorizer(max_features=2500,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c8eb9",
   "metadata": {},
   "source": [
    "#### \n",
    "When you used BOW or TF-IDF:\n",
    "\n",
    "You had a cleaned corpus → tokenized text.\n",
    "\n",
    "You directly fed it into CountVectorizer or TfidfVectorizer.\n",
    "\n",
    "These vectorizers automatically handle:\n",
    "\n",
    "Vocabulary building\n",
    "\n",
    "Conversion of text → numerical matrix (sparse matrix)\n",
    "\n",
    "Aligning train and test data into same feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will fit and transform the X_train data and transform the X_test data\n",
    "X_train = cv.fit_transform(X_train).toarray()\n",
    "X_test = cv.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the vocabulary of the BOW model and the index of each word in the feature vector\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MultinomialNB = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "spam_detect_model=MultinomialNB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6991c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for test data\n",
    "y_pred=spam_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred)\n",
    "print(f\"confusion matrix\\n: {confusion_mtx}\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\" Accuracy : {accuracy}\")\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f\" Precision : {precision}\")\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\" Recall : {recall}\")\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\" F1 Score : {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e474554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
